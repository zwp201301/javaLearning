#分布式技术原理与实战45讲

你好，我是邴越，在一线互联网公司从事分布式开发工作多年，一直关注分布式理论和新技术的发展。



互联网发展到今天，用户数量越来越多，产生的数据规模也越来越大，应用系统必须支持高并发访问和海量数据处理的需求。



对比集中式架构，分布式系统由于具有可扩展性，可以动态扩展服务和存储节点，使用廉价的机器构建高性能的服务，更适合如今的互联网业务。分布式系统技术已经成为微服务架构、大数据、云计算等技术领域的基石，在电商、互联网金融、支付等众多业务中，都离不开分布式技术的有效运用。



掌握分布式技能的后端工程师越来越抢手，不止业务部门、中间件和基础架构等部门也在大规模抢人。分布式技术的应用越来越广泛，各大公司的相关岗位要求也越来越高，然而在面试和工作中，我们却看到各种各样的问题：

- 面试时，可以回答概念性的问题，但问到实质性问题时就懵了，由于缺少相关经验而卡住；

- 工作中对常用分布式技术的原理一知半解，在典型场景下可以应付，但稍微变更业务场景或业务目标后，就开始毫无头绪；

- 系统设计中，没有全面平衡各个设计点，关注了收益，却没考虑到风险，比如增加了缓存，却带来了数据不一致，增加了消息队列，却因为不合理的重试导致服务异常。



总结来说，这往往是从业者没有在实际的分布式业务场景中实践过，或者对分步式技术缺乏体系化的认知，或者对一些原理和底层的内容未曾深入研究，导致可以解决常见问题，而没有系统化的解决思路。



因此，我梳理了一套分布式技术的方法论，希望可以帮助你快速而体系化地补齐分布式知识。此外，一路走来，我在分布式系统设计中踩过的坑，在开发实践中看到和经历过的一些典型问题，也将在这里一并分享给你，希望能够帮到更多开发者，并减轻你学习分布式的畏难心理。

分布式是工程师进阶的必经之路

经常听到一些开发人员，在工作之余感叹自己职业发展的困惑与焦虑，比如每天写业务代码，如何摆脱 CRUD Boy 的标签，去提升技术能力？一直在传统企业工作，怎么才能加入 BAT 等大公司？所有的机遇都是在充分准备后才能获得的，这些问题的关键，就是你在技术上的持续精进。

![img](https://s0.lgstatic.com/i/image3/M01/07/63/Ciqah16ERqmAE-qtAAJ5c5hckiA055.png)

如果想在技术线上深耕和谋求发展，成为高级工程师、资深工程师或者架构师，掌握分布式系统知识已经成为了必要的一环。不管是目前流行的 SOA 架构，还是蓬勃发展的微服务和 Serverless 架构，都是在分布式的基础上构建的，业务开发中的框架选型、注册中心，以及服务拆分之后面临的分布式事务问题、分布式锁，也都是分布式系统所关注的。

## 想要高薪 Offer，必须掌握分布式

要想进入大公司并拿到高薪 Offer，分布式技术也是一个很好的敲门砖。大型互联网公司每天都要面对海量的业务请求，处理各种复杂的系统问题是工作常态，所以需要应聘人员掌握常用的分布式技术，并在面试过程中重点考察你对分布式系统的理解和经验水平。



针对高级岗位，除了掌握在分布式环境下进行开发的能力，你还需要了解其中的原理、机制，以便能够快速定位线上问题；而对于架构师来说，你还需要具备独立设计分布式系统的能力，这就需要了解高并发、高可用的相关知识了。



在拉勾网上搜索后端工程师的招聘岗位，可以看到很多岗位都要求掌握缓存、分布式服务、消息队列等分布式组件应用，部分岗位还要求在高并发等分布式设计方向有一定的积累。

![img](https://s0.lgstatic.com/i/image3/M01/08/6A/Ciqah16FzVKAUHomABFJwSsmtFg192.png)



结合拉勾对海量招聘启事的大数据分析，我们也总结出了后端开发者在面试中要求掌握的分布式技能点，同时也把它们融入到了课程设计中：

- 分布式系统理论和设计；

- 分布式事务和一致性；

-  分布式服务及微服务架构；

- 分布式缓存和常见 NoSQL 应用；

- 分布式下数据库的拆分，比如读写分离、分库分表；

- 消息中间件的应用，常见组件的选型；

- 合理应用分布式技术，实现系统的高可用。

![img](https://s0.lgstatic.com/i/image3/M01/80/79/Cgq2xl6ERqmAEq1kAAENcRtXEvU094.png)



难点不难，给你学得会的分布式课程

分布式系统在工作和面试中如此重要，但是掌握起来并不容易。



- 理论众多、难以入手。分布式系统不仅涉及一致性、事务等众多的理论知识，还包括非常多的复杂算法，比如 Paxos 和 Zab 算法，如果没有一个明确的抓手，学习起来会很吃力。



- 领域庞杂、关联技术栈多。分布式系统涉及很多领域，比如 RPC 服务调用、分库分表，这些不同的领域需要了解和掌握不同的技术栈。因此我的建议是，要想快速提升分布式技术能力，那么需要明确哪些才是你日常工作中最迫切需要的，从实践中开始体验和学习，积累经验。要知道，分布式不是一堆理论的堆砌，而是和日常开发息息相关。



- 工作特点，接触不到分布式。鉴于现在一些软件开发公司，或者传统公司的 IT 部门，还在使用集中式系统架构，所以部分开发者平时在工作中很少接触分布式系统，因此，我在这个课程中，将会侧重讲解很多实际场景的实践内容，以帮助你更有效地掌握分布式。



工作多年，我从一个初入行的新人，一步步晋升一线互联网公司的核心业务负责人，我深知分布式知识的重要性和学习痛点，为了让你在短时间内能够快速掌握分布式知识，我对这门课程进行了精心设计。



（1）知识体系化，快速学习

碎片化知识很难有效学习，体系化的学习才是重点。分布式系统知识足够庞杂，本课程从理论开始，一步一步落地到实践中，帮助你快速构建知识框架，让你对分布式技术有个总体的认知。

![img](https://s0.lgstatic.com/i/image3/M01/07/63/Ciqah16ERqmAGJjpAACXHV15Oyg347.png)

（2）选取最常用的知识点

分布式系统博大精深，但并不是每个人都在做基础架构研发，也不是每一项技术都能直接落地，因而本课程选取了在工程开发中最常用的技术栈，比如在分布式服务模块中选取了网关、注册中心、容器化等内容来讲解，在数据库模块中选择了读写分离、分库分表等内容，这些都是在开发中打交道最多的知识点。



（3）拒绝空谈理论，结合实际业务场景

技术是为业务服务的，再高深的技术都要落地，我们的课程内容不是干巴巴的讲理论，而是结合了实际业务场景，带着问题去讲解，让你能够在实际的场景中理解并应用，达到事半功倍的效果。



（4）面试真题解析，帮你赢取高薪 Offer

为了帮助你更好地准备面试，每个模块后面都附上了一个“加餐”内容，并梳理出了面试中经常出现的考点，以及高频面试真题。虽然是加餐，但是内容绝对有料。



当然，快速通关面试只是我们的目标之一，我更希望你在这个课程中，真正学有所得，将知识和经验融入到个人能力中，做一些长期主义的事情。

##课程设计

本课程分为 7 个模块，共 45 讲。我将从实际工作和面试出发，从分布式理论开始带你建立知识框架，然后逐个攻破分布式技术的各个核心技术领域。为了让你更清晰地了解本课程中的所有知识点，我还准备了一份思维导图：

![img](https://s0.lgstatic.com/i/image3/M01/80/79/Cgq2xl6ERqmAdmMXAAMdZN_Jn7I815.png)

- 分布式基础：扎实的理论是进一步学习分布式知识的钥匙，这一模块将详解分布式的概念，包括 CAP 和 Base 理论、各种数据一致性模型，以及两阶段和三阶段提交协议等。

- 分布式事务：在电商、金融等业务中都涉及资金往来，事务非常重要，那么分布式事务如何解决、分布式锁如何实现、……，这一模块将会解答。

- 分布式服务：分布式服务是微服务架构的必要条件，这一模块将讲解如何解决服务拆分后的一系列问题，比如 RPC、网关、注册中心等。

- 分布式存储：系统架构拆分以后，存储层面的拆分同样重要，数据库层涉及读写分离、分库分表等，这一模块我们来一起来探究这些技术的原理，以及如何在业务中落地。

- 消息队列：消息中间件是分布式系统架构的整合剂，这一模块将分享消息队列使用的常见问题，比如重复消费、消息时序等。

- 分布式缓存： 缓存的高性能在分布式系统中发挥了更加重要的作用，那么分布式缓存有哪些分类，以及有哪些经典问题，这一模块我们来一起探究。

- 分布式高可用：高可用是工程师始终追求的目标，最后这个模块，我将会为你分享在分布式系统中如何保障系统可用性，如何做好系统监控和限流降级。

**写在最后**

这个课程，我从面试出发，利用贴近工作实战的内容来为你梳理知识体系，希望无论是对分布式技术感兴趣，还是正在准备面试的工程师，都能够轻松掌握分布式技术。专栏中贴近实战经验和方法论，也一定会让从事分布式开发的你，找到答案或得到启发。当然，如果你是即将面临求职季的学生，如果能了解一些分布式知识，相信你一定能在校园招聘中得到面试官的更多青睐。



对于用户来说，学习专栏是自我提升的方式；对于作者来说，打磨一个好的专栏，高质量地输出是另一种方式的提高。希望我们在这个课程结束时，都能给自己一份满意的答卷。



#第01讲：如何证明分布式系统的 CAP 理论？

本课时我们主要介绍分布式系统中最基础的 CAP 理论及其应用。



对于开发或设计分布式系统的架构师、工程师来说，CAP 是必须要掌握的基础理论，CAP 理论可以帮助架构师对系统设计中目标进行取舍，合理的规划系统拆分的维度。下面我们先讲讲分布式系统的特点。

##分布式系统的特点

随着移动互联网的快速发展，互联网的用户数量越来越多，产生的数据规模也越来越大，对应用系统提出了更高的要求，我们的系统必须支持高并发访问和海量数据处理。



分布式系统技术就是用来解决集中式架构的性能瓶颈问题，来适应快速发展的业务规模，一般来说，分布式系统是建立在网络之上的硬件或者软件系统，彼此之间通过消息等方式进行通信和协调。



** 分布式系统的核心是可扩展性 **，通过对服务、存储的扩展，来提高系统的处理能力，通过对多台服务器协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务。



除了对可扩展性的需求，分布式系统还有**不出现单点故障**、服务或者存储**无状态**等特点。

单点故障（Single Point Failure）是指在系统中某个组件一旦失效，这会让整个系统无法工作，而不出现单点故障，单点不影响整体，就是分布式系统的设计目标之一；

无状态，是因为无状态的服务才能满足部分机器宕机不影响全部，可以随时进行扩展的需求。



由于分布式系统的特点，在分布式环境中更容易出现问题，比如节点之间通信失败、网络分区故障、多个副本的数据不一致等，为了更好的在分布式系统下进行开发，学者们提出了一系列的理论，其中具有代表性的就是 CAP 理论。

## CAP 代表什么含义

CAP 理论可以表述为，一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）这三项中的两项。

![img](https://s0.lgstatic.com/i/image3/M01/07/64/Ciqah16ER_SAGmCqAADG3jNX34o901.png)

一致性是指“所有节点同时看到相同的数据”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，等同于所有节点拥有数据的最新版本。



可用性是指“任何时候，读写都是成功的”，即服务一直可用，而且是正常响应时间。我们平时会看到一些 IT 公司的对外宣传，比如系统稳定性已经做到 3 个 9、4 个 9，即 99.9%、99.99%，这里的 N 个 9 就是对可用性的一个描述，叫做 SLA，即服务水平协议。比如我们说月度 99.95% 的 SLA，则意味着每个月服务出现故障的时间只能占总时间的 0.05%，如果这个月是 30 天，那么就是 21.6 分钟。



分区容忍性具体是指“当部分节点出现消息丢失或者分区故障的时候，分布式系统仍然能够继续运行”，即系统容忍网络出现分区，并且在遇到某节点或网络分区之间网络不可达的情况下，仍然能够对外提供满足一致性和可用性的服务。



在分布式系统中，由于系统的各层拆分，P 是确定的，CAP 的应用模型就是 CP 架构和 AP 架构。分布式系统所关注的，就是在 Partition Tolerance 的前提下，如何实现更好的 A，和更稳定的 C。

## CAP 理论的证明

CAP 理论的证明有多种方式，通过反证的方式是最直观的。反证法来证明 CAP 定理，最早是由 Lynch 提出的，通过一个实际场景，如果 CAP 三者可同时满足，由于允许 P 的存在，则一定存在 Server 之间的丢包，如此则不能保证 C。

![img](https://s0.lgstatic.com/i/image3/M01/80/7B/Cgq2xl6ER_SAIiA0AACyIE8xkbY529.png)首先构造一个单机系统，如上图，Client A 可以发送指令到 Server 并且设置更新 X 的值，Client 1 从 Server 读取该值，在单点情况下，即没有网络分区的情况下，通过简单的事务机制，可以保证 Client 1 读到的始终是最新值，不存在一致性的问题。

![img](https://s0.lgstatic.com/i/image3/M01/07/64/Ciqah16ER_SAbt2BAAGrjnzOmj0352.png)我们在系统中增加一组节点，因为允许分区容错，Write 操作可能在 Server 1 上成功，在 Server 2 上失败，这时候对于 Client 1 和 Client 2，就会读取到不一致的值，出现不一致的情况。如果要保持 X 值的一致性，Write 操作必须同时失败， 也就是降低系统的可用性。



可以看到，在分布式系统中，无法同时满足 CAP 定律中的“一致性”、“可用性”和“分区容错性”三者。



在该证明中，对 CAP 的定义进行了更明确的声明：

- Consistency，一致性被称为原子对象，任何的读写都应该看起来是“原子“的，或串行的，写后面的读一定能读到前面写的内容，所有的读写请求都好像被全局排序；

- Availability，对任何非失败节点都应该在有限时间内给出请求的回应（请求的可终止性）；

- Partition Tolerance，允许节点之间丢失任意多的消息，当网络分区发生时，节点之间的消息可能会完全丢失。

##  CAP 理论的应用

CAP 理论提醒我们，在架构设计中，不要把精力浪费在如何设计能满足三者的完美分布式系统上，而要合理进行取舍，CAP 理论类似数学上的不可能三角，只能三者选其二，不能全部获得。



不同业务对于一致性的要求是不同的。举个例来讲，在微博上发表评论和点赞，用户对不一致是不敏感的，可以容忍相对较长时间的不一致，只要做好本地的交互，并不会影响用户体验；而我们在电商购物时，产品价格数据则是要求强一致性的，如果商家更改价格不能实时生效，则会对交易成功率有非常大的影响。



需要注意的是，CAP 理论中是忽略网络延迟的，也就是当事务提交时，节点间的数据复制一定是需要花费时间的。即使是同一个机房，从节点 A 复制到节点 B，由于现实中网络不是实时的，所以总会有一定的时间不一致。

## CP 和 AP 架构的取舍

在通常的分布式系统中，为了保证数据的高可用，通常会将数据保留多个副本（Replica），网络分区是既成的现实，于是只能在可用性和一致性两者间做出选择。CAP 理论关注的是在绝对情况下，在工程上，可用性和一致性并不是完全对立的，我们关注的往往是如何**在保持相对一致性的前提下，提高系统的可用性。**



业务上对一致性的要求会直接反映在系统设计中，典型的就是 CP 和 AP 结构。



- CP 架构：对于 CP 来说，放弃可用性，追求一致性和分区容错性。

我们熟悉的 ZooKeeper，就是采用了 CP 一致性，ZooKeeper 是一个分布式的服务框架，主要用来解决分布式集群中应用系统的协调和一致性问题。其核心算法是 Zab，所有设计都是为了一致性。在 CAP 模型中，ZooKeeper 是 CP，这意味着面对网络分区时，为了保持一致性，它是不可用的。关于 Zab 协议，将会在后面的 ZooKeeper 课时中介绍。



- AP 架构：对于 AP 来说，放弃强一致性，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的 Base 也是根据 AP 来扩展的。

和 ZooKeeper 相对的是 Eureka，Eureka 是 Spring Cloud 微服务技术栈中的服务发现组件，Eureka 的各个节点都是平等的，几个节点挂掉不影响正常节点的工作，剩余的节点依然可以提供注册和查询服务，只要有一台 Eureka 还在，就能保证注册服务可用，只不过查到的信息可能不是最新的版本，不保证一致性。

**总结**

这一课时分享了分布式系统的基础——CAP 理论，包括 CAP 分别代表什么含义、如何证明、CAP 不同模型的典型代表，以及 CAP 在系统设计中有哪些应用。

#第02讲：不同数据一致性模型有哪些应用？

本课时我们主要讲解“不同数据一致性模型有哪些应用”？



上一课时讲过，对于 CAP 来说，放弃强一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择。在工程实践中，基于 CAP 定理逐步演化，就提出了 Base 理论。



那么 Base 理论有哪些内容，Base 理论下的一致性模型又有哪些呢？

##Base 理论

Base 是三个短语的简写，即基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）。

![img](https://s0.lgstatic.com/i/image3/M01/08/52/Ciqah16FrueAWLATAABOyQi2X3M251.png)

Base 理论的核心思想是最终一致性，即使无法做到强一致性（Strong Consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual Consistency）。



接下来我们着重对 Base 理论中的三要素进行讲解。

##  三个要素详解

###  基本可用

基本可用比较好理解，就是不追求 CAP 中的「任何时候，读写都是成功的」，而是系统能够基本运行，一直提供服务。基本可用强调了分布式系统在出现不可预知故障的时候，允许损失部分可用性，相比正常的系统，可能是响应时间延长，或者是服务被降级。



举个例子，在双十一秒杀活动中，如果抢购人数太多超过了系统的 QPS 峰值，可能会排队或者提示限流，这就是通过合理的手段保护系统的稳定性，保证主要的服务正常，保证基本可用。

![img](https://s0.lgstatic.com/i/image3/M01/08/52/Ciqah16FrueAMh29AANTN6izVWY035.png)

### 软状态

软状态可以对应 ACID 事务中的原子性，在 ACID 的事务中，实现的是强制一致性，要么全做要么不做，所有用户看到的数据一致。其中的原子性（Atomicity）要求多个节点的数据副本都是一致的，强调数据的一致性。



原子性可以理解为一种“硬状态”，软状态则是允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。

### 最终一致性

数据不可能一直是软状态，必须在一个时间期限之后达到各个节点的一致性，在期限过后，应当保证所有副本保持数据一致性，也就是达到数据的最终一致性。



在系统设计中，最终一致性实现的时间取决于网络延时、系统负载、不同的存储选型、不同数据复制方案设计等因素。

## 全局时钟和逻辑时钟

接下来我会分析不同数据一致性模型的分类，在这之前，我们先来看一个分布式系统中的全局时钟概念。



分布式系统解决了**传统单体架构的单点问题和性能容量问题**，另一方面也带来了很多新的问题，其中一个问题就是多节点的时间同步问题：不同机器上的物理时钟难以同步，导致无法区分在分布式系统中多个节点的事件时序。



没有全局时钟，绝对的内部一致性是没有意义的，一般来说，我们讨论的一致性都是外部一致性，而外部一致性主要指的是多并发访问时更新过的数据如何获取的问题。



和全局时钟相对的，是**逻辑时钟**，逻辑时钟描绘了分布式系统中事件发生的时序，是为了区分现实中的物理时钟提出来的概念。



一般情况下我们提到的时间都是指物理时间，但实际上很多应用中，只要所有机器有相同的时间就够了，这个时间不一定要跟实际时间相同。更进一步解释：如果两个节点之间不进行交互，那么它们的时间甚至都不需要同步。 因此问题的关键点在于**节点间的交互要在事件的发生顺序上达成一致，而不是对于时间达成一致。**



逻辑时钟的概念也被用来解决分布式一致性问题，这里我们不展开，感兴趣的可以找一些相关的资料来学习。

## 不同数据一致性模型

一般来说，数据一致性模型可以分为强一致性和弱一致性，强一致性也叫做线性一致性，除此以外，所有其他的一致性都是弱一致性的特殊情况。弱一致性根据不同的业务场景，又可以分解为更细分的模型，不同一致性模型又有不同的应用场景。



在互联网领域的绝大多数场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

![img](https://s0.lgstatic.com/i/image3/M01/08/52/Ciqah16FruiAGz3eAAIrOBxKnpU229.png)

对于一致性，可以分为从服务端和客户端两个不同的视角，上面提到了全局时钟概念，这里关注的主要是外部一致性。

### 强一致性

当更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。根据 CAP 理论，这种实现需要牺牲可用性。

###  弱一致性

系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。

### 最终一致性

最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。



到达最终一致性的时间 ，就是不一致窗口时间，在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。



最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括**因果一致性**和**会话一致性**等。



### 因果一致性

因果一致性要求有因果关系的操作顺序得到保证，非因果关系的操作顺序则无所谓。



进程 A 在更新完某个数据项后通知了进程 B，那么进程 B 之后对该数据项的访问都应该能够获取到进程 A 更新后的最新值，并且如果进程 B 要对该数据项进行更新操作的话，务必基于进程 A 更新后的最新值。



因果一致性的应用场景可以举个例子，在微博或者微信进行评论的时候，比如你在朋友圈发了一张照片，朋友给你评论了，而你对朋友的评论进行了回复，这条朋友圈的显示中，你的回复必须在朋友之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致。



### 会话一致性

会话一致性将对系统数据的访问过程框定在了一个会话当中，约定了系统能保证在同一个有效的会话中实现“读己之所写”的一致性，就是在你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。



实际开发中有分布式的 Session 一致性问题，可以认为是会话一致性的一个应用。

## CAP 及 Base 的关系

Base 理论是在 CAP 上发展的，CAP 理论描述了分布式系统中数据一致性、可用性、分区容错性之间的制约关系，当你选择了其中的两个时，就不得不对剩下的一个做一定程度的牺牲。



Base 理论则是对 CAP 理论的实际应用，也就是在分区和副本存在的前提下，通过一定的系统设计方案，放弃强一致性，实现基本可用，这是大部分分布式系统的选择，比如 NoSQL 系统、微服务架构。在这个前提下，如何把基本可用做到最好，就是分布式工程师们追求的，在这个课程中，我们也会有专门的模块来讲解高可用。



除了 CAP 和 Base，上面还提到了 ACID 原理，ACID 是一种强一致性模型，强调原子性、一致性、隔离性和持久性，主要用于在数据库实现中。Base 理论面向的是高可用、可扩展的分布式系统，ACID 适合传统金融等业务，在实际场景中，不同业务对数据的一致性要求不一样，ACID 和 Base 理论往往会结合使用。

**总结**

这一课时分析了 Base 理论和不同的数据一致性模型，其内容比较抽象，特别是逻辑时钟和一致性部分，如果你有充裕的时间，建议找一些扩展资料来学习。

#第03讲：如何透彻理解 Paxos 算法？

本课时我们主要讲解“如何透彻理解 Paxos 算法”？



Paxos 算法在分布式领域具有非常重要的地位，开源分布式锁组件 Google Chubby 的作者 Mike Burrows 说过，**这个世界上只有一种一致性算法**，那就是 Paxos 算法，其他的算法都是残次品。



Paxos 算法虽然重要，但是也因算法复杂而著名，不过 Paxos 算法是学习分布式系统必需的一个知识点，这一课时我们就知难而上，一起来学习下 Paxos 算法。

## Quorum 机制

在学习 Paxos 算法之前，我们先来看分布式系统中的 Quorum 选举算法。在各种一致性算法中都可以看到Quorum 机制的身影，主要数学思想来源于抽屉原理，用一句话解释那就是，在 N 个副本中，一次更新成功的如果有 W 个，那么我在读取数据时是要从大于 N－W 个副本中读取，这样就能至少读到一个更新的数据了。



和 Quorum 机制对应的是 WARO，也就是**Write All Read one**，是一种简单的副本控制协议，当 Client 请求向某副本写数据时（更新数据），只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。



WARO 优先保证读服务，因为所有的副本更新成功，才能视为更新成功，从而保证了

所有的副本一致，这样的话，只需要读任何一个副本上的数据即可。写服务的可用性较低，因为只要有一个副本更新失败，此次写操作就视为失败了。假设有 N 个副本，N－1 个都宕机了，剩下的那个副本仍能提供读服务；但是只要有一个副本宕机了，写服务就不会成功。



WARO 牺牲了更新服务的可用性，最大程度地增强了读服务的可用性，而 Quorum 就是在更新服务和读服务之间进行的一个折衷。

## Quorum 定义

Quorum 的定义如下：假设有 N 个副本，更新操作 wi 在 W 个副本中更新成功之后，才认为此次更新操作 wi 成功，把这次成功提交的更新操作对应的数据叫做：“成功提交的数据”。对于读操作而言，至少需要读 R 个副本才能读到此次更新的数据，其中，W+R>N ，即 W 和 R 有重叠，一般，W+R=N+1。

> N = 存储数据副本的数量

> W = 更新成功所需的副本

> R = 一次数据对象读取要访问的副本的数量

Quorum就是限定了一次需要读取至少N+1-w的副本数据,听起来有些抽象，举个例子，我们维护了10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据，可以保证我们读到了最新的数据。

## Quorum 的应用

Quorum 机制无法保证强一致性，也就是无法实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据。



Quorum 机制的使用需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据。



Quorum 是分布式系统中常用的一种机制，用来保证数据冗余和最终一致性的投票算法，在 Paxos、Raft 和 ZooKeeper 的 Zab 等算法中，都可以看到 Quorum 机制的应用。

## Paxos 节点的角色和交互

了解了 Quorum 机制，我们接下来学习 Paxos 算法，首先看一下 Paxos 算法中的节点角色和交互。

###  Paxos 的节点角色

在 Paxos 协议中，有三类节点角色，分别是 Proposer、Acceptor 和 Learner，另外还有一个 Client，作为产生议题者。

![img](https://s0.lgstatic.com/i/image3/M01/84/0C/Cgq2xl6MNF2AHbQiAABGDsfyB3s143.png)

上述三类角色只是逻辑上的划分，在工作实践中，一个节点可以同时充当这三类角色。

#### Proposer 提案者

Proposer 可以有多个，在流程开始时，Proposer 提出议案，也就是value，所谓 value，在工程中可以是任何操作，比如“修改某个变量的值为某个新值”，Paxos 协议中统一将这些操作抽象为 value。



不同的 Proposer 可以提出不同的甚至矛盾的 value，比如某个 Proposer 提议“将变量 X 设置为 1”，另一个 Proposer 提议“将变量 X 设置为 2”，但对同一轮 Paxos 过程，最多只有一个 value 被批准。

#### Acceptor 批准者

在集群中，Acceptor 有 N 个，Acceptor 之间完全对等独立，Proposer 提出的 value 必须获得超过半数（N/2+1）的 Acceptor 批准后才能通过。

#### Learner 学习者

Learner 不参与选举，而是学习被批准的 value，在Paxos中，Learner主要参与相关的状态机同步流程。

这里Leaner的流程就参考了Quorum 议会机制，某个 value 需要获得 W=N/2 + 1 的 Acceptor 批准，Learner 需要至少读取 N/2+1 个 Accpetor，最多读取 N 个 Acceptor 的结果后，才能学习到一个通过的 value。

### Client 产生议题者

Client 角色，作为产生议题者，实际不参与选举过程，比如发起修改请求的来源等。

Proposer 与 Acceptor 之间的交互

Paxos 中， Proposer 和 Acceptor 是算法核心角色，Paxos 描述的就是在一个由多个 Proposer 和多个 Acceptor 构成的系统中，如何让多个 Acceptor 针对 Proposer 提出的多种提案达成一致的过程，而 Learner 只是“学习”最终被批准的提案。



Proposer 与 Acceptor 之间的交互主要有 4 类消息通信，如下图：

![img](https://s0.lgstatic.com/i/image3/M01/0A/F6/Ciqah16MNF2Ad_j9AAA5-uz9BWI899.png)

这 4 类消息对应于 Paxos 算法的两个阶段 4 个过程，下面在分析选举过程时会讲到。

### Paxos 选举过程

选举过程可以分为两个部分，准备阶段和选举阶段，可以查看下面的时序图：

![img](https://s0.lgstatic.com/i/image3/M01/84/0C/Cgq2xl6MNF2ASwyyAAE2bn8RiaM148.png)

#### Phase 1 准备阶段

Proposer 生成全局唯一且递增的 ProposalID，向 Paxos 集群的所有机器发送 Prepare 请求，这里不携带 value，只携带 N 即 ProposalID。



Acceptor 收到 Prepare 请求后，判断收到的 ProposalID 是否比之前已响应的所有提案的 N 大，如果是，则：

在本地持久化 N，可记为 Max_N；

回复请求，并带上已经 Accept 的提案中 N 最大的 value，如果此时还没有已经 Accept 的提案，则返回 value 为空；

做出承诺，不会 Accept 任何小于 Max_N 的提案。



如果否，则不回复或者回复 Error。

#### Phase 2 选举阶段

为了方便描述，我们把 Phase 2 选举阶段继续拆分为 P2a、P2b 和 P2c。

> P2a：Proposer 发送 Accept

经过一段时间后，Proposer 收集到一些 Prepare 回复，有下列几种情况：

-  若回复数量 > 一半的 Acceptor 数量，且所有回复的 value 都为空时，则 Porposer 发出 accept 请求，并带上自己指定的 value。

- 若回复数量 > 一半的 Acceptor 数量，且有的回复 value 不为空时，则 Porposer 发出 accept 请求，并带上回复中 ProposalID 最大的 value，作为自己的提案内容。

- 若回复数量 <= 一半的 Acceptor 数量时，则尝试更新生成更大的 ProposalID，再转到准备阶段执行。

> P2b：Acceptor 应答 Accept

Accpetor 收到 Accpet 请求 后，判断：

- 若收到的 N >= Max_N（一般情况下是等于），则回复提交成功，并持久化 N 和 value；

- 若收到的 N < Max_N，则不回复或者回复提交失败。

>P2c: Proposer 统计投票

经过一段时间后，Proposer 会收集到一些 Accept 回复提交成功的情况，比如：

- 当回复数量 > 一半的 Acceptor 数量时，则表示提交 value 成功，此时可以发一个广播给所有的 Proposer、Learner，通知它们已 commit 的 value；

- 当回复数量 <= 一半的 Acceptor 数量时，则尝试更新生成更大的 ProposalID，转到准备阶段执行。

- 当收到一条提交失败的回复时，则尝试更新生成更大的 ProposalID，也会转到准备阶段执行。

### Paxos 常见的问题

关于Paxos协议，有几个常见的问题，简单介绍下。



#### 1.如果半数以内的 Acceptor 失效，如何正常运行？

在Paxos流程中，如果出现半数以内的 Acceptor 失效，可以分为两种情况：

第一种，如果半数以内的 Acceptor 失效时还没确定最终的 value，此时所有的 Proposer 会重新竞争提案，最终有一个提案会成功提交。

第二种，如果半数以内的 Acceptor 失效时已确定最终的 value，此时所有的 Proposer 提交前必须以最终的 value 提交，也就是Value实际已经生效，此值可以被获取，并不再修改。



#### 2.Acceptor需要接受更大的N，也就是ProposalID有什么意义？

这种机制可以防止其中一个Proposer崩溃宕机产生阻塞问题，允许其他Proposer用更大ProposalID来抢占临时的访问权。



#### 3.如何产生唯一的编号，也就是 ProposalID？

在《Paxos made simple》论文中提到，唯一编号是让所有的 Proposer 都从不相交的数据集合中进行选择，需要保证在不同Proposer之间不重复，比如系统有 5 个 Proposer，则可为每一个 Proposer 分配一个标识 j(0~4)，那么每一个 Proposer 每次提出决议的编号可以为 5*i + j，i 可以用来表示提出议案的次数。

**总结**

这一课时分享了 Paxos 协议相关的知识点，Paxos 是经典的分布式协议，理解了它们以后，学习其他分布式协议会简单很多。

Paxos算法更重要的是理解过程，并不是要把各个流程都背下来，除了文中介绍的，相关的分支判断和选择场景还有很多，如果希望了解Paxos算法相关的推导和证明，我在最后附上了 Paxos 相关的几篇论文地址，感兴趣的同学可以去学习下：

The PartTime Parliament

Paxos Made Simple

fast-paxos

#第04讲：ZooKeeper 如何保证数据一致性？

在分布式场景中，ZooKeeper 的应用非常广泛，比如数据发布和订阅、命名服务、配置中心、注册中心、分布式锁等。



ZooKeeper 提供了一个类似于 Linux 文件系统的数据模型，和基于 Watcher 机制的分布式事件通知，这些特性都依赖 ZooKeeper 的高容错数据一致性协议。



那么问题来了，在分布式场景下，ZooKeeper 是如何实现数据一致性的呢？

##Zab 一致性协议

ZooKeeper 是通过 Zab 协议来保证分布式事务的最终一致性。Zab（ZooKeeper Atomic Broadcast，ZooKeeper 原子广播协议）支持崩溃恢复，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间数据一致性。



系统架构可以参考下面这张图：

![img](https://s0.lgstatic.com/i/image3/M01/0C/DF/Ciqah16O5QyAB4rJAAEiJ-4T3bE046.png)

在 ZooKeeper 集群中，所有客户端的请求都是写入到 Leader 进程中的，然后，由 Leader 同步到其他节点，称为 Follower。在集群数据同步的过程中，如果出现 Follower 节点崩溃或者 Leader 进程崩溃时，都会通过 Zab 协议来保证数据一致性。



Zab 协议的具体实现可以分为以下两部分：



- 消息广播阶段

Leader 节点接受事务提交，并且将新的 Proposal 请求广播给 Follower 节点，收集各个节点的反馈，决定是否进行 Commit，在这个过程中，也会使用上一课时提到的 Quorum 选举机制。



- 崩溃恢复阶段

如果在同步过程中出现 Leader 节点宕机，会进入崩溃恢复阶段，重新进行 Leader 选举，崩溃恢复阶段还包含数据同步操作，同步集群中最新的数据，保持集群的数据一致性。



整个 ZooKeeper 集群的一致性保证就是在上面两个状态之前切换，当 Leader 服务正常时，就是正常的消息广播模式；当 Leader 不可用时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段。

##Zab 协议中的 Zxid

Zxid 在 ZooKeeper 的一致性流程中非常重要，在详细分析 Zab 协议之前，先来看下 Zxid 的概念。



Zxid 是 Zab 协议的一个事务编号，Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增计数器，针对客户端每一个事务请求，计数器加 1；而高 32 位则代表 Leader 周期年代的编号。



这里 Leader 周期的英文是 epoch，可以理解为当前集群所处的年代或者周期，对比另外一个一致性算法 Raft 中的 Term 概念。在 Raft 中，每一个任期的开始都是一次选举，Raft 算法保证在给定的一个任期最多只有一个领导人。

![img](https://s0.lgstatic.com/i/image3/M01/85/F5/Cgq2xl6O5QyAeZqMAAB5C-BWbeI425.png)

Zab 协议的实现也类似，每当有一个新的 Leader 选举出现时，就会从这个 Leader 服务器上取出其本地日志中最大事务的 Zxid，并从中读取 epoch 值，然后加 1，以此作为新的周期 ID。总结一下，高 32 位代表了每代 Leader 的唯一性，低 32 位则代表了每代 Leader 中事务的唯一性。

##Zab 流程分析

Zab 的具体流程可以拆分为消息广播、崩溃恢复和数据同步三个过程，下面我们分别进行分析。

![img](https://s0.lgstatic.com/i/image3/M01/0C/DF/Ciqah16O5QyADv0LAAA84x9hlQc078.png)

## 消息广播

在 ZooKeeper 中所有的事务请求都由 Leader 节点来处理，其他服务器为 Follower，Leader 将客户端的事务请求转换为事务 Proposal，并且将 Proposal 分发给集群中其他所有的 Follower。



完成广播之后，Leader 等待 Follwer 反馈，当有过半数的 Follower 反馈信息后，Leader 将再次向集群内 Follower 广播 Commit 信息，Commit 信息就是确认将之前的 Proposal 提交。



这里的 Commit 可以对比 SQL 中的 COMMIT 操作来理解，MySQL 默认操作模式是 autocommit 自动提交模式，如果你显式地开始一个事务，在每次变更之后都要通过 COMMIT 语句来确认，将更改提交到数据库中。



Leader 节点的写入也是一个两步操作，第一步是广播事务操作，第二步是广播提交操作，其中过半数指的是反馈的节点数 >=N/2+1，N 是全部的 Follower 节点数量。



消息广播的过程描述可以参考下图：

![img](https://s0.lgstatic.com/i/image3/M01/85/F5/Cgq2xl6O5Q2ASjMpAAHdAdF67vE736.png)

客户端的写请求进来之后，Leader 会将写请求包装成 Proposal 事务，并添加一个递增事务 ID，也就是 Zxid，Zxid 是单调递增的，以保证每个消息的先后顺序；

广播这个 Proposal 事务，Leader 节点和 Follower 节点是解耦的，通信都会经过一个先进先出的消息队列，Leader 会为每一个 Follower 服务器分配一个单独的 FIFO 队列，然后把 Proposal 放到队列中；

Follower 节点收到对应的 Proposal 之后会把它持久到磁盘上，当完全写入之后，发一个 ACK 给 Leader；

当 Leader 收到超过半数 Follower 机器的 ack 之后，会提交本地机器上的事务，同时开始广播 commit， Follower 收到 commit 之后，完成各自的事务提交。

分析完消息广播，我们再来看一下崩溃恢复。

## 崩溃恢复

消息广播通过 Quorum 机制，解决了 Follower 节点宕机的情况，但是如果在广播过程中 Leader 节点崩溃呢？



这就需要 Zab 协议支持的崩溃恢复，崩溃恢复可以保证在 Leader 进程崩溃的时候可以重新选出 Leader，并且保证数据的完整性。



崩溃恢复和集群启动时的选举过程是一致的，也就是说，下面的几种情况都会进入崩溃恢复阶段：

- 初始化集群，刚刚启动的时候

- Leader 崩溃，因为故障宕机

- Leader 失去了半数的机器支持，与集群中超过一半的节点断连



崩溃恢复模式将会开启新的一轮选举，选举产生的 Leader 会与过半的 Follower 进行同步，使数据一致，当与过半的机器同步完成后，就退出恢复模式， 然后进入消息广播模式。



Zab 中的节点有三种状态，伴随着的 Zab 不同阶段的转换，节点状态也在变化：

![img](https://s0.lgstatic.com/i/image3/M01/85/F8/Cgq2xl6O5-SASu1cAABBzFJh3s0114.png)

我们通过一个模拟的例子，来了解崩溃恢复阶段，也就是选举的流程。



假设正在运行的集群有五台 Follower 服务器，编号分别是 Server1、Server2、Server3、Server4、Server5，当前 Leader 是 Server2，若某一时刻 Leader 挂了，此时便开始 Leader 选举。



选举过程如下：



1.各个节点变更状态，变更为 Looking

ZooKeeper 中除了 Leader 和 Follower，还有 Observer 节点，Observer 不参与选举，Leader 挂后，余下的 Follower 节点都会将自己的状态变更为 Looking，然后开始进入 Leader 选举过程。



2.各个 Server 节点都会发出一个投票，参与选举

在第一次投票中，所有的 Server 都会投自己，然后各自将投票发送给集群中所有机器，在运行期间，每个服务器上的 Zxid 大概率不同。



3.集群接收来自各个服务器的投票，开始处理投票和选举

处理投票的过程就是对比 Zxid 的过程，假定 Server3 的 Zxid 最大，Server1 判断 Server3 可以成为 Leader，那么 Server1 就投票给 Server3，判断的依据如下：

> 首先选举 epoch 最大的

> 如果 epoch 相等，则选 zxid 最大的

> 若 epoch 和 zxid 都相等，则选择 server id 最大的，就是配置 zoo.cfg 中的 myid



在选举过程中，如果有节点获得超过半数的投票数，则会成为 Leader 节点，反之则重新投票选举。

![img](https://s0.lgstatic.com/i/image3/M01/0C/DF/Ciqah16O5Q2AL03bAADZjhkw3ys031.png)

4.选举成功，改变服务器的状态，参考上面这张图的状态变更

##数据同步

崩溃恢复完成选举以后，接下来的工作就是数据同步，在选举过程中，通过投票已经确认 Leader 服务器是最大Zxid 的节点，同步阶段就是利用 Leader 前一阶段获得的最新Proposal历史，同步集群中所有的副本。



上面分析了 Zab 协议的具体流程，接下来我们对比一下 Zab 协议和 Paxos 算法。

##  Zab 与 Paxos 算法的联系与区别

Paxos 的思想在很多分布式组件中都可以看到，Zab 协议可以认为是基于 Paxos 算法实现的，先来看下两者之间的联系：

- 都存在一个 Leader 进程的角色，负责协调多个 Follower 进程的运行

- 都应用 Quorum 机制，Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交

- 在 Zab 协议中，Zxid 中通过 epoch 来代表当前 Leader 周期，在 Paxos 算法中，同样存在这样一个标识，叫做 Ballot Number



两者之间的区别是，Paxos 是理论，Zab 是实践，Paxos 是论文性质的，目的是设计一种通用的分布式一致性算法，而 Zab 协议应用在 ZooKeeper 中，是一个特别设计的崩溃可恢复的原子消息广播算法。



Zab 协议增加了崩溃恢复的功能，当 Leader 服务器不可用，或者已经半数以上节点失去联系时，ZooKeeper 会进入恢复模式选举新的 Leader 服务器，使集群达到一个一致的状态。

**总结**

这一课时的内容分享了 ZooKeeper 一致性实现，包括 Zab 协议中的 Zxid 结构，Zab 协议具体的流程实现，以及 Zab 和原生 Paxos 算法的区别和联系。



Zab 协议在实际处理中有很多的实现细节，由于篇幅原因，这里只分享了核心的流程，若对该协议感兴趣的话，可以在课后继续找些书籍或者资料来学习：

《从Paxos到Zookeeper》

《ZooKeeper:分布式过程协同技术详解》

#第05讲：共识问题：区块链如何确认记账权？

本课时我们主要讲解“共识问题：区块链如何确认记账权？”



区块链可以说是最近几年最热的技术领域之一，区块链起源于中本聪的比特币，作为比特币的底层技术，本质上是一个去中心化的数据库，其特点是去中心化、公开透明，作为分布式账本技术，每个节点都可以参与数据库的记录。



区块链是一个注重安全和可信度胜过效率的一项技术，如果说互联网技术解决的是通讯问题，区块链技术解决的则是信任问题。



今天我们关注区块链中的核心问题：作为分布式账本，每个参与者都维护了一份数据，那么如何确认记账权，最终的账本以谁为准呢？

##区块链的共识

区块链是一种去中心化的分布式账本系统，区块链的共识问题实际上是来源于分布式系统的一致性问题。



共识（Consensus）故名思义，共同的认识，共识问题研究的是多个成员如何达成一致，典型的比如投票选举。



共识机制在区块链中扮演着核心的地位，共识机制决定了谁有记账的权利，以及记账权利的选择过程和理由。不同的虚拟货币采用共识机制也不同，常见的共识机制有 POW、POS、DPOS等。



我们前面提到 CAP 中的 C 是 Consistency（一致性），Consistency 和 Consensus 有什么区别呢？



Consistency 侧重的是内容在时间顺序上的一致和统一，而 Consensus 则是指由许多参与者对某项内容达成共识，所以一般把 Consistency 翻译为“一致性”，把 Consensus 翻译为“共识”。

##拜占庭将军问题

前面的课程中我们已经分享了几个经典的一致性算法，如果把共识机制延伸到分布式系统中，就是系统需要有一个主进程来协调，系统的所有决定都由主进程来达成一致性。



到了区块链中，由于区块链是一种去中心化的分布式系统，所以区块链中是没有类似于团队里的领导，以及分布式系统中的 master 角色，这样就需要有某种共识机制，以便保证系统一致性。



一般在网络通信中，把节点故障，也就是信道不可靠的情况称为“非拜占庭错误”，恶意响应，也就是系统被攻击，传递错误消息称为“拜占庭错误”。



为什么叫拜占庭错误呢？实际上是来自于一个故事模型：



拜占庭帝国就是中世纪的土耳其帝国，拥有巨大的财富，周围 10 个邻邦垂诞已久，但拜占庭高墙耸立，固若金汤，没有一个单独的邻邦能够成功入侵。任何单个邻邦入侵都会失败，同时也有可能自身被其他 9 个邻邦入侵。

拜占庭帝国防御能力如此之强，至少要有十个邻邦中的一半以上同时进攻，才有可能攻破。然而，如果其中的一个或者几个邻邦本身答应好一起进攻，但实际过程出现背叛，那么入侵者可能都会被歼灭。

于是每一方都小心行事，不敢轻易相信邻国，这就是拜占庭将军问题。



在拜占庭问题里，各邻国最重要的事情是：所有将军如何能过达成共识去攻打拜占庭帝国。拜占庭将军问题核心描述是军中可能有叛徒，却要保证进攻一致，由此引申到计算机领域，发展成了一种容错理论：



一群将军想要实现某一个目标，比如一致进攻或者一致撤退，单独行动是行不通的，必须合作，达成共识；由于叛徒的存在，将军们不知道应该如何达到一致。


其实，拜占庭将军问题（Byzantine Generals Problem）和我们前面提到的 Paxos 算法、逻辑时钟等，都是由 Leslie Lamport 提出的。



Lamport 可以说是分布式系统的奠基人之一，由于在分布式领域的一系列贡献，Lamport 获得了 2013 年的图灵奖。




拜占庭将军问题和我们今天要讨论的记账权有什么联系呢？



在记账权的归属中，关键的是如何避免恶意共识的出现，也就是错误的记账，类似如何处理拜占庭将军中的叛徒。



比特币是区块链技术最广泛的应用，在比特币中如何决定记账权呢？答案就是 POW 机制，接下来我们分析 POW 工作量证明机制。

##POW 工作量证明

PoW（Proof of Work，工作量证明）被认为是经过验证最安全的拜占庭解决机制，最早是用来防垃圾邮件的，典型的就是 Google 邮箱的反垃圾邮件系统。



Google 邮箱强制要求每一个给 Google 服务器发送邮件的发送者，必须先完成一定量的计算工作，造成一小段时间的延迟，比如延迟 1 秒，如果是正常的邮件发送，这个时间是可以接受；如果是广告邮件发送者，因为要进行大量的发送工作，这种无价值的计算是无法忍受的。

##挖矿的由来

挖矿是比特币系统中一个形象化的表达，那么挖矿是怎么来的呢？



比特币挖矿是将一段时间内比特币系统中发生的交易进行确认，并记录在区块链上形成新区块的过程，由于需要竞争记账权，利用计算机去计算 Hash 数值，随机碰撞解题，这个过程就是挖矿。



换句话说，就是比特币系统出一道数学题，大家抢答最优解，挖矿就是记账的过程，矿工是记账员，区块链就是账本。

##比特币的 POW 实现

比特币中的 POW 实现，是通过计算来猜测一个数值（Nonce），得以解决规定的 Hash 问题，下面是比特币的区块结构，可以看到区块头有个随机数字段，这个就是 Nonce 值：

![img](https://s0.lgstatic.com/i/image3/M01/0F/56/Ciqah16VcOeATZ5AAARuh8uNMzo929.png)

中本聪在比特币系统中设置了一道题目，通过不断调节 Nonce 的值，来对区块头算 Hash，要求找到一个 Nonce 值，使得算出来的 Hash 值满足某个固定值。



具体的 Hash 方法一般是使用 SHA256 算法 ，你可以查看这个小工具来测试 https://tool.oschina.net/encrypt?type=2。



我们来简化一下计算过程，假设第 100 个区块给出的区块值是下列字符串，最早计算出该字符串的节点可以获得比特币：

> f7684590e9c732fb3cf4bf0b8e0f5ea9511e8bbaacb589892634ae7938e5700c



由于 Hash 算法是一个不可以逆的算法，没法通过具体的 Hash 值倒推出原文，这样每个节点只能采用穷举的方法，也就是选择各种字符串，比如开始的 a、b、c、1、2、3、…，不断的尝试。



比特币系统自身会调节难度，控制解题的时间，一般来讲，约每 10 分钟挖出一个区块，在这 10 分钟内，计算机只能不停地去计算，去试各种字符串。



这个过程实际上是考验各个节点的服务器性能，也就是算力。如果你算力非常强大，有几万台服务器，可以很快得到 Nonce 值，也就是正确答案：lagou，对应 Hash 值和题目要求一致。



接下来你就可以把这个 Nonce 值放在结构体里，通过 P2P 网络广播出去，其他的系统节点收到后，发现这个 Nonce 值是合法的，能满足要求，会认为你挖矿成功。



由于解出了题目，你会获得系统对应的比特币奖励，以及本区块内所有交易产生的手续费。其他节点发现有人已经算出来了，就会放弃本次计算，然后开启下一个区块的题目，去寻找下一个区块头的 Nonce 值。



作为落地的最大区块链系统，比特币的区块信息一直在动态生成。下面这张截图记录了比特币最近的区块信息，区块信息来自 https://www.blockchain.com/，你也可以点击链接查看最新的区块高度。

![img](https://s0.lgstatic.com/i/image3/M01/88/6C/Cgq2xl6VcOeAW3HxAAQnHJsTFFY002.png)

##区块链分叉和 51% 攻击

Hash 问题具有不可逆的特点，主要依靠暴力计算，谁的算力多，谁最先解决问题的概率就越大。当掌握超过全网一半算力时，就能控制网络中链的走向，这也是所谓 51% 攻击的由来。



前面我们说过，因为区块链中每个节点都可以参与记账，系统中可能出现链的分叉（Fork），最终会有一条链成为最长的链。



但是在现实社会中，牵扯到参与各方巨大的利益关系，算力之间的博弈往往并没有那么简单，以比特币为例，已经分裂出了 BCH（比特币现金）、BTG（比特币黄金）等分叉币。

##POW 机制优缺点

POW 的优点有很多，POW 是第一个完全实现去中心化共识算法的，并且节点自由进出，容易实现，由于对算力的高要求，破坏系统花费的成本也巨大。



POW 机制的缺点也是显而易见的，最大的就是浪费能源，巨大的算力被浪费在了无尽的挖矿行为中，并且本身并不产生任何价值。



这也是区块链被很多人指责的一点，浪费了大量的能源，收获的仅仅是一堆无价值的数据存储，换个角度来思考，这也说明了在去中心化的场景下，实现信任是多么的困难。



另一方面也可以看到，大量的数字货币矿场都是建设在西南地区的深山中，利用当地价格低廉的电力资源，或者就直接和发电站建设在一起。

##其他共识方法

除了 POW 机制，还有其他的共识方法，典型的就是 POS 和 DPOS 等。

##POS 权益证明

POS（Proof of Stake，权益证明）类似现实生活中的股东大会机制，拥有股份越多的人拥有越多的投票权，也就越容易获取记账权。


POS 是通过保证金来对赌一个合法的块成为新的区块，收益为抵押资本的利息和交易服务费。提供证明的保证金越多，则获得记账权的概率就越大，合法记账者可以获得收益。著名的数字货币 ETH（以太坊），就在共识算法中采用了 POS 机制。

##DPOS 委托权益证明

采用 DPOS（Delegated Proof of Stake，委托权益证明）机制的典型代表是 EOS，如果说 POS 类似股东大会，比较的是谁持有的股份多，那么 DPOS 类似于公司董事会制度，在 DPOS 共识制度下，会选出一定数量的代表，来负责生产区块。

**总结**

区块链可以说是分布式系统最广泛的应用之一，今天介绍了区块链共识问题的由来、拜占庭将军问题，以及典型的 POW 解决机制。



区块链是一个非常广的主题，以拜占庭将军问题为例，在区块链之前，还有两种解决方案：口头协议和书面协议。专栏的内容是以分布式系统为主，后面的章节不会继续区块链相关话题的讨论，如果你希望了解更多的区块链技术及应用，下面附上了一些相关的资料：

- 《区块链开发指南》

- 《区块链核心算法解析》

- 比特币的区块结构解析

- 区块链技术发展及应用场景

- The Byzantine Generals Problem

- https://www.8btc.com/

#加餐1：如何准备一线互联网公司面试？

本课时我们来讲讲如何准备一线互联网公司面试。

##互联网技术面试的特点
互联网公司的技术面试有一些侧重点，国内互联网公司和外企的侧重点又有不同。BAT 互联网公司看重项目能力，重点考察语言深度和项目能力，国外 IT 公司更看重计算机基础，比如微软和 Amazon 的面试，每轮面试都是算法题的在线测评，针对社招还会有 System Design 题目。

一般来说，一线互联网公司面试都有下面的特点：

![image (2).png](https://s0.lgstatic.com/i/image3/M01/07/3E/CgoCgV6hUdCAHsw6AAC5j_lXmsI267.png)

1. 看重数据结构和算法等计算机基础知识

一线互联网公司在面试中更加关注计算机基础知识的考察，比如数据结构和算法，操作系统、网络原理，目前，很多国内公司在招聘上也看齐 Google、Facebook 等海外企业，面试重点考察算法，如果没有 ACM 经验，不刷题很难通过。

2. 深入技术栈，考察对原理和源码的掌握程度

深入底层实现，考察对相关组件的原理掌握程度，以及是否读过源码等。因为互联网用户基数比较大，一个细微的优化可能会带来很大的收益，同样，一个很小的问题可能会对线上业务造成毁灭性的影响，所以要知其然还要知其所以然，对技术栈的掌握要求比较深入。

3. 偏向实际问题，考察业务中的应用

面试中通常会结合实际业务场景来提问，其考察的是在真实业务中如何设计。我们知道，条条大路通罗马，一个功能点，技术方案可能有很多，但是从落地到代码实现，就要限制于整体方案、上下游约束等，典型的比如秒杀系统、微博会员关注关系设计等。

4. 重视分布式系统、高可用等设计方向

大型互联网公司，特别是 C 端的业务，面对的是海量的用户和请求，牵一发而动全身，对系统可用性、分布式高可用等有极高的要求，所以在面试中会重点考察分布式系统设计，如何构建高并发高可用的系统。

##如何高效准备面试
分析了互联网公司的面试特点，接下来就是有针对性的准备面试。如何快速了解一个公司的招聘要求？答案就是去各大招聘网站，从招聘启事入手，这也是最快、最有效的方式。

我们总结了招聘网站上十几家互联网大厂的招聘启事，从中寻找共性，以 Java 语言为代表，把一线互联网公司后端工程师通用的招聘要求列在了下面：

计算机以及相关专业，本科或以上学历；
扎实的数据结构与算法基础，熟悉计算机及网络相关知识；
熟悉 Java 开发，掌握面向对象思想，具备扎实的抽象能力、设计能力；
熟练使用 Spring 或其他 Web 框架，了解其原理；
熟练使用 MySQL、Redis、 MongoDB 或者 ElasticSearch 等存储技术；
了解 JVM 内存管理，掌握 JVM 调优技能；
熟悉分布式系统常见技术，掌握 RPC 框架和微服务架构；
良好的沟通交流能力，具备较强的学习能力和责任心，可以编写良好的代码文档。
感兴趣的可以去招聘网站上看一下，对后端开发的要求，基本就是在这个范围里，从这个通用招聘要求上，我们可以逐条拆解，总结如何高效准备面试。

![image (3).png](https://s0.lgstatic.com/i/image3/M01/07/3E/CgoCgV6hUdmAVkBQAADSNFZJjQU555.png)

1. 对学历和专业的要求，硬性标准

对学历和专业的要求，这一条一般都会注明，不过计算机行业比较包容，不拘一格，非科班以及转专业的技术大牛也有很多，这里不展开。

2. 加强计算机基础，提高算法和数据结构、操作系统等底层能力

计算机基础能力是面试的重点，在校招中更是着重考察。

数据结构方面，基本的数组、栈和队列、字符串、二叉树等结构，比如二叉树是面试中的重点，手写红黑树有点夸张，不过基本的遍历、二叉树重建、二叉树深度等必须掌握，需要在白纸上写写代码，考的是白板编程能力。

算法方面，基本的排序和查找、递归、分治、动态规划之类都会考察，这方面可以多看看《剑指 offer》《编程珠玑》，国内推荐牛客网，国外就是 LeetCode 的高频题。

操作系统和网络原理，比如基本的调度算法、文件系统，还有各种网络协议，比如 TCP/IP 协议、拥塞控制等。操作系统推荐机械工业出版社的华章系列教材，网络原理也有一些经典书籍，如果觉得《TCP/IP 详解》太厚，可以看《图解 HTTP 协议》和《图解 TCP/IP 协议》。

3. 深入一门编程语言，了解底层实现，各种语法糖和特性

后端工程师不管学习多少语言，都要有一门自己的主编程语言，什么是主编程语言，就是对这个编程语言你可以达到精通的程度，不是只会用，要从代码编译开始就知道程序是怎么运行的。典型的主语言有 Java、C++、PHP 及 Python 等。

针对 Java 语言，要了解 Java 语言的底层机制，字节码怎么用，为什么 Java 是平台无关型语言，这些都要搞明白，应用层面，对集合框架、网络 IO、并发编程、泛型、异常、反射等技术都要有比较深入的了解，一些常见的组件，还要学习源码，优化层面，Java 虚拟机调优、常见 JVM 问题的处理，这些都是面试经常考察的，也是一定要掌握的。

4. 加强数据库和缓存应用，掌握 NoSQL 技术

数据存储是业务的基石，从关系型数据库 MySQL 到 NoSQL，从 Memcached 到 Redis 的各种缓存，这些都是面试的必考题，从应用到底层逻辑都必须了解，数据库本身这块的知识点更是重要，Redis 也是面试的重点，作为应用最多的缓存，Redis 在开发中已经和 MySQL 一样重要。

5. 学习高并发和高可用的分布式系统设计

高并发是技术人一直追求的，为什么我们说双十一是对系统架构的挑战，就是天量的 QPS 请求，在这种情况下，如何保障系统的高可用，保证业务正常，是每个工程师都要思考的。分布式系统架构，以及高并发和高可用知识，则需要在工作中注意积累，如果工作中没有类似的上手锻炼机会，也可以通过各种书籍和专栏等渠道来学习。

6. 增强软性指标，包括快速学习，良好的沟通能力

除了技术实力，软性指标也很重要，在平时的工作中，要注意梳理文档，养成良好的文档能力，和同事的沟通中，多学习下《金字塔原理》等沟通技巧，在面试中就可以更好的表现自己。



另外，要注意工作上业务的连续性，技术为业务服务，更好地了解业务，也可以帮助你拿到心仪的 Offer。

